"""
LinkedIn Job Scraper using Bright Data Scraping Browser

A production-ready LinkedIn job scraper that integrates with Bright Data's
Scraping Browser service using Puppeteer-over-WebSocket connections.
Based on the official Bright Data implementation patterns.

Reference: https://github.com/luminati-io/bright-data-scraping-browser-nodejs-puppeteer-project
"""

import asyncio
import json
import subprocess
import tempfile
import os
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
import structlog
from urllib.parse import urlencode
import re

from app.core.config import settings

logger = structlog.get_logger()


class LinkedInScraperV2:
    """
    LinkedIn job scraper using Bright Data's Scraping Browser with Puppeteer.
    
    This implementation follows the official Bright Data patterns for
    browser automation via WebSocket connections, using Node.js + Puppeteer
    execution through subprocess calls.
    """
    
    def __init__(self):
        self.websocket_endpoint = self._build_websocket_endpoint()
        self.rate_limit_delay = 2.0
        self.last_request_time = None
        
    def _build_websocket_endpoint(self) -> Optional[str]:
        """Build the Bright Data Scraping Browser WebSocket endpoint."""
        if not settings.BRIGHT_DATA_ENDPOINT:
            logger.warning("Bright Data WebSocket endpoint not configured")
            return None
            
        # The endpoint should be in WebSocket format:
        # wss://brd-customer-[id]-zone-[zone]:[password]@[domain]:[port]
        return settings.BRIGHT_DATA_ENDPOINT
    
    async def search_jobs(
        self,
        keywords: str,
        location: str = "",
        experience_level: str = "",
        job_type: str = "",
        date_posted: str = "week",
        limit: int = 25
    ) -> List[Dict[str, Any]]:
        """
        Search for jobs on LinkedIn using Bright Data Scraping Browser.
        """
        if not self.websocket_endpoint:
            raise Exception("Bright Data Scraping Browser is not configured")
        
        await self._rate_limit()
        
        try:
            logger.info("Starting LinkedIn job search", 
                       keywords=keywords, location=location, limit=limit)
            
            search_url = self._build_linkedin_search_url(
                keywords, location, experience_level, job_type, date_posted
            )
            
            jobs = await self._scrape_with_puppeteer(search_url, limit)
            
            validated_jobs = self._validate_and_clean_jobs(jobs)
            
            logger.info(f"Successfully scraped {len(validated_jobs)} jobs from LinkedIn")
            return validated_jobs
            
        except Exception as e:
            logger.error("LinkedIn job search failed", error=str(e), exc_info=True)
            raise e

    async def _scrape_with_puppeteer(self, search_url: str, limit: int) -> List[Dict[str, Any]]:
        """
        Use Bright Data's Scraping Browser with Puppeteer to scrape LinkedIn jobs.
        """
        try:
            # Generate the Puppeteer script
            script_content = self._generate_puppeteer_script(search_url, limit)
            
            # Execute the script using Node.js
            jobs = await self._execute_nodejs_script(script_content)
            
            return jobs if jobs else []
            
        except Exception as e:
            logger.error(f"Puppeteer scraping failed: {e}")
            return []

    async def _execute_nodejs_script(self, script_content: str) -> List[Dict[str, Any]]:
        """
        Execute the Puppeteer script using Node.js subprocess.
        """
        try:
            # Get the backend directory where node_modules is located
            backend_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            
            # Create a temporary file for the script in the backend directory
            temp_file_path = os.path.join(backend_dir, f"temp_scraper_{os.getpid()}.js")
            
            with open(temp_file_path, 'w') as temp_file:
                temp_file.write(script_content)
            
            logger.info("Executing Puppeteer script via Node.js")
            
            # Execute the Node.js script from the backend directory
            process = await asyncio.create_subprocess_exec(
                'node', temp_file_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=backend_dir
            )
            
            stdout, stderr = await process.communicate()
            
            # Clean up the temporary file
            try:
                os.unlink(temp_file_path)
            except OSError:
                pass  # File might already be deleted
            
            if process.returncode == 0:
                try:
                    # Parse the JSON output
                    result = json.loads(stdout.decode())
                    if isinstance(result, list):
                        logger.info(f"Successfully extracted {len(result)} jobs via Puppeteer")
                        return result
                    else:
                        logger.warning("Unexpected result format from Puppeteer script")
                        return []
                except json.JSONDecodeError as e:
                    logger.error(f"Failed to parse Puppeteer output: {e}")
                    logger.error(f"Raw output: {stdout.decode()}")
                    return []
            else:
                logger.error(f"Puppeteer script failed with return code {process.returncode}")
                logger.error(f"Error output: {stderr.decode()}")
                return []
                
        except FileNotFoundError:
            logger.error("Node.js not found. Please install Node.js to use Puppeteer scraping")
            return []
        except Exception as e:
            logger.error(f"Error executing Node.js script: {e}")
            return []

    def _generate_puppeteer_script(self, search_url: str, limit: int) -> str:
        """
        Generate a complete Puppeteer script for Node.js execution.
        Based on the official Bright Data implementation patterns.
        """
        return f"""
const puppeteer = require('puppeteer-core');

async function scrapeLinkedIn() {{
    console.error('üöÄ Starting LinkedIn scraper...');
    
    let browser;
    try {{
        // Connect to Bright Data's Scraping Browser
        console.error('üåê Connecting to Bright Data Scraping Browser...');
        browser = await puppeteer.connect({{
            browserWSEndpoint: '{self.websocket_endpoint}'
        }});
        console.error('‚úÖ Connected to browser');
        
        // Create a new page
        const page = await browser.newPage();
        
        // Set a realistic user agent
        await page.setUserAgent('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36');
        
        // Navigate to LinkedIn job search
        console.error('üåê Navigating to LinkedIn...');
        await page.goto('{search_url}', {{ 
            waitUntil: 'networkidle2', 
            timeout: 60000 
        }});
        console.error('‚úÖ Page loaded');
        
        // Wait for job cards to appear
        console.error('‚è≥ Waiting for job cards to load...');
        try {{
            await page.waitForSelector('.job-search-card, .base-card, [data-testid="job-card"]', {{ 
                timeout: 30000 
            }});
            console.error('‚úÖ Job cards found');
        }} catch (e) {{
            console.error('‚ö†Ô∏è No job cards found, proceeding anyway...');
        }}
        
        // Scroll to load more jobs
        console.error('üìú Scrolling to load more jobs...');
        for (let i = 0; i < 3; i++) {{
            await page.evaluate(() => window.scrollTo(0, document.body.scrollHeight));
            await page.waitForTimeout(2000);
        }}
        
        // Extract job data
        console.error('üìä Extracting job data...');
        const jobs = await page.evaluate((limit) => {{
            const jobCards = document.querySelectorAll(
                '.job-search-card, .base-card, [data-testid="job-card"], .base-search-card'
            );
            const results = [];
            
            console.log(`Found ${{jobCards.length}} job cards`);
            
            for (let i = 0; i < Math.min(jobCards.length, limit); i++) {{
                const card = jobCards[i];
                
                try {{
                    // Extract job details using multiple selectors
                    const titleSelectors = [
                        '.base-search-card__title',
                        '.job-search-card__title', 
                        '[data-testid="job-title"]',
                        '.base-card__title'
                    ];
                    
                    const companySelectors = [
                        '.base-search-card__subtitle',
                        '.job-search-card__subtitle',
                        '[data-testid="job-company"]',
                        '.base-card__subtitle'
                    ];
                    
                    const locationSelectors = [
                        '.job-search-card__location',
                        '[data-testid="job-location"]',
                        '.base-card__location'
                    ];
                    
                    const urlSelectors = [
                        '.base-card__full-link',
                        '.job-search-card__title-link',
                        '[data-testid="job-title-link"]'
                    ];
                    
                    const descriptionSelectors = [
                        '.job-search-card__snippet',
                        '.base-card__snippet',
                        '[data-testid="job-description"]'
                    ];
                    
                    const dateSelectors = [
                        'time',
                        '.job-search-card__listdate',
                        '[data-testid="job-posted-date"]'
                    ];
                    
                    // Helper function to get text from multiple selectors
                    const getTextFromSelectors = (selectors) => {{
                        for (const selector of selectors) {{
                            const element = card.querySelector(selector);
                            if (element) {{
                                return element.textContent?.trim() || '';
                            }}
                        }}
                        return '';
                    }};
                    
                    // Helper function to get href from multiple selectors
                    const getHrefFromSelectors = (selectors) => {{
                        for (const selector of selectors) {{
                            const element = card.querySelector(selector);
                            if (element && element.href) {{
                                return element.href;
                            }}
                        }}
                        return '';
                    }};
                    
                    const title = getTextFromSelectors(titleSelectors);
                    const company = getTextFromSelectors(companySelectors);
                    const location = getTextFromSelectors(locationSelectors);
                    const url = getHrefFromSelectors(urlSelectors);
                    const description = getTextFromSelectors(descriptionSelectors);
                    const postedAt = getTextFromSelectors(dateSelectors);
                    
                    // Only include jobs with essential data
                    if (title && company && url) {{
                        results.push({{
                            id: `linkedin_${{url.split('?')[0].split('/').pop()}}`,
                            title: title,
                            company: company,
                            location: location || 'Not specified',
                            url: url.split('?')[0], // Remove tracking parameters
                            description: description,
                            posted_at: postedAt,
                            source: 'linkedin',
                            scraped_at: new Date().toISOString()
                        }});
                    }}
                }} catch (e) {{
                    console.log(`Error processing job card ${{i}}: ${{e.message}}`);
                }}
            }}
            
            console.log(`Extracted ${{results.length}} valid jobs`);
            return results;
        }}, {limit});
        
        console.error(`‚úÖ Extracted ${{jobs.length}} jobs`);
        
        // Output the results as JSON
        console.log(JSON.stringify(jobs));
        
    }} catch (error) {{
        console.error('‚ùå Error occurred:', error.message);
        console.log(JSON.stringify([])); // Return empty array on error
    }} finally {{
        if (browser) {{
            await browser.close();
            console.error('üëã Browser closed');
        }}
    }}
}}

// Run the scraper
scrapeLinkedIn();
"""

    def _build_linkedin_search_url(
        self, 
        keywords: str, 
        location: str, 
        experience_level: str, 
        job_type: str, 
        date_posted: str
    ) -> str:
        """Build LinkedIn job search URL with proper filters."""
        base_url = "https://www.linkedin.com/jobs/search"
        params = {
            "keywords": keywords,
            "location": location,
            "f_TPR": "r604800"  # Default to past week
        }

        # Map date posted to LinkedIn's format
        if date_posted:
            date_map = {
                "day": "r86400",      # Past 24 hours
                "week": "r604800",    # Past week  
                "month": "r2592000"   # Past month
            }
            params["f_TPR"] = date_map.get(date_posted.lower(), "r604800")

        # Map experience level
        if experience_level:
            exp_map = {
                "entry": "1",
                "associate": "2", 
                "mid": "3,4",
                "senior": "4,5",
                "director": "5,6",
                "executive": "6"
            }
            exp_value = exp_map.get(experience_level.lower())
            if exp_value:
                params["f_E"] = exp_value

        # Map job type
        if job_type:
            type_map = {
                "full-time": "F",
                "part-time": "P",
                "contract": "C", 
                "temporary": "T",
                "internship": "I"
            }
            type_value = type_map.get(job_type.lower())
            if type_value:
                params["f_JT"] = type_value
        
        # Remove None values before encoding
        params = {k: v for k, v in params.items() if v}
        
        search_url = f"{base_url}?{urlencode(params)}"
        logger.debug("Built LinkedIn search URL", url=search_url)
        return search_url

    def _validate_and_clean_jobs(self, jobs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Validate and clean extracted job data."""
        validated_jobs = []
        seen_urls = set()
        
        for job in jobs:
            try:
                job_url = job.get("url", "")
                if not job_url or job_url in seen_urls:
                    continue
                
                cleaned_job = {
                    "id": self._clean_text(job.get("id", "")),
                    "title": self._clean_text(job.get("title", "")),
                    "company": self._clean_text(job.get("company", "")),
                    "location": self._clean_text(job.get("location", "Not specified")),
                    "url": job_url.split('?')[0],  # Remove tracking params
                    "description": self._clean_text(job.get("description", "")),
                    "source": "linkedin",
                    "posted_at": self._parse_date(job.get("posted_at")),
                    "scraped_at": datetime.now().isoformat()
                }
                
                # Only add if we have essential fields
                if cleaned_job["title"] and cleaned_job["company"]:
                    validated_jobs.append(cleaned_job)
                    seen_urls.add(job_url)
                    
            except Exception as e:
                logger.warning(f"Error processing job: {e}")
                continue
        
        logger.info(f"Validated {len(validated_jobs)} out of {len(jobs)} jobs")
        return validated_jobs

    def _clean_text(self, text: Optional[str]) -> Optional[str]:
        """Clean and normalize text content."""
        if not text:
            return None
        return re.sub(r'\s+', ' ', str(text).strip())

    def _parse_date(self, date_str: Optional[str]) -> Optional[str]:
        """Parse relative dates like '2 days ago' into ISO format."""
        if not date_str:
            return None
        
        try:
            date_str = str(date_str).lower()
            now = datetime.now()
            
            if "hour" in date_str:
                hours = int(re.search(r'\d+', date_str).group())
                return (now - timedelta(hours=hours)).isoformat()
            elif "day" in date_str:
                days = int(re.search(r'\d+', date_str).group())
                return (now - timedelta(days=days)).isoformat()
            elif "week" in date_str:
                weeks = int(re.search(r'\d+', date_str).group())
                return (now - timedelta(weeks=weeks)).isoformat()
            elif "month" in date_str:
                months = int(re.search(r'\d+', date_str).group())
                return (now - timedelta(days=months * 30)).isoformat()
                
        except Exception as e:
            logger.warning(f"Could not parse date: {date_str}, error: {e}")
            
        return datetime.now().isoformat()

    async def _rate_limit(self):
        """Ensure a minimum delay between requests."""
        if self.last_request_time:
            elapsed = datetime.now().timestamp() - self.last_request_time
            if elapsed < self.rate_limit_delay:
                await asyncio.sleep(self.rate_limit_delay - elapsed)
        self.last_request_time = datetime.now().timestamp()

    async def close(self):
        """Close any resources."""
        logger.info("LinkedIn scraper session closed")

# Global instance for use across the application
linkedin_scraper_v2 = LinkedInScraperV2()