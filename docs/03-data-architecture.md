# Data Architecture Specification

## Overview

This document outlines the data architecture for the Smart Assistant, which employs a **hybrid data model**. This model combines a relational database for structured data, a vector database for semantic data, and a caching layer for performance. This approach ensures data integrity, enables powerful semantic search capabilities, and provides a responsive user experience.

## Core Principles

1.  **Polyglot Persistence**: Use the best storage technology for each type of data. Structured, transactional data lives in PostgreSQL, while unstructured, semantic data lives in Qdrant.
2.  **Data Integrity**: The relational database serves as the single source of truth for all core business objects, with strong consistency guarantees for user-initiated actions.
3.  **Semantic-First**: All relevant data points are represented as vector embeddings in Qdrant to enable contextual understanding and intelligent retrieval across the entire system.
4.  **Privacy by Design**: All personally identifiable information (PII) is handled with strict access controls and is encrypted at rest and in transit.
5.  **Performance through Caching**: A Redis caching layer is used to store the results of expensive queries or API calls, minimizing latency and database load.

## Data Storage Layers

### 1. Relational Database (PostgreSQL)
-   **Purpose**: The primary store for structured, transactional data. It is the source of truth for core entities like users, jobs, and emails.
-   **Key Responsibilities**:
    -   Storing user accounts, profiles, and preferences.
    -   Managing the state of job opportunities, email classifications, and content generation history.
    -   Enforcing data integrity through constraints and relationships.
    -   Handling transactional operations to ensure atomicity.

### 2. Vector Database (Qdrant)
-   **Purpose**: The engine for semantic search and long-term memory. It allows agents to understand the *meaning* and *context* of data, not just its literal content.
-   **Key Responsibilities**:
    -   Storing vector embeddings of all significant data points (e.g., user skills, job descriptions, email content, news articles).
    -   Providing a fast and scalable semantic search capability for all agents.
    -   Enabling the `ContextManager` to build a rich, contextual understanding of the user and their activities.

### 3. Caching Layer (Redis)
-   **Purpose**: To improve application performance and reduce the load on the primary databases.
-   **Key Responsibilities**:
    -   Caching frequently accessed, non-critical data (e.g., user context objects, results of external API calls).
    -   Serving as the backend for a distributed task queue (e.g., Celery) to manage asynchronous agent tasks.

## Logical Data Model & Schemas

This section defines the logical structure of the data entities stored in the PostgreSQL database.

### `users`
-   **Purpose**: Stores core user account information.
-   **Fields**: `id`, `email`, `created_at`, `preferences` (JSONB for storing UI settings, etc.).

### `career_profiles`
-   **Purpose**: Stores the user's professional profile.
-   **Fields**: `id`, `user_id`, `title`, `skills` (array), `experience_years`, `salary_preferences`, `location_preferences`.

### `job_opportunities`
-   **Purpose**: Tracks all discovered job opportunities for a user.
-   **Fields**: `id`, `user_id`, `title`, `company`, `location`, `description`, `source`, `relevance_score`, `status` (e.g., 'new', 'applied', 'archived').

### `email_classifications`
-   **Purpose**: Stores the results of the `EmailAgent`'s processing.
-   **Fields**: `id`, `user_id`, `gmail_message_id`, `sender`, `subject`, `category` (e.g., 'Career', 'Financial'), `priority_level`, `summary`.

### `content_generations`
-   **Purpose**: Logs all content generated by the `ContentAgent`.
-   **Fields**: `id`, `user_id`, `content_type` (e.g., 'CV', 'Cover Letter'), `target_id` (e.g., a job ID), `generated_content`, `quality_score`, `user_feedback`.

### `news_articles`
-   **Purpose**: A centralized table to store all curated news articles.
-   **Fields**: `id`, `title`, `url` (unique), `summary`, `source`, `published_at`.

### `user_briefings`
-   **Purpose**: Stores the personalized briefings generated for the user.
-   **Fields**: `id`, `user_id`, `briefing_date`, `content` (JSONB), `user_engagement` (JSONB for tracking clicks, etc.).

### `agent_tasks`
-   **Purpose**: Tracks the state and outcome of all asynchronous agent tasks.
-   **Fields**: `id`, `user_id`, `agent_name`, `task_type`, `status` (e.g., 'pending', 'completed', 'failed'), `input_data`, `output_data`.

## Vector Data Strategy (Qdrant)

### Collections
The Qdrant database will be organized into collections based on the type of data being embedded.

1.  **`user_memories`**:
    -   **Content**: Stores embeddings of the user's profile, preferences, goals, and interaction history.
    -   **Purpose**: Provides a holistic, semantic understanding of the user for all agents.

2.  **`documents`**:
    -   **Content**: Stores chunked embeddings of large text documents, such as CVs, job descriptions, and news articles.
    -   **Purpose**: Enables deep semantic search and retrieval within and across documents.

3.  **`conversations`**:
    -   **Content**: Stores embeddings of user interactions and agent responses.
    -   **Purpose**: Provides conversational memory, allowing agents to understand the history of their interactions with the user.

### Embedding and Retrieval Process
1.  **Embedding**: When new data is created or updated in PostgreSQL (e.g., a new job is found), a corresponding process is triggered to generate a vector embedding of its key fields. This embedding is then stored in the appropriate Qdrant collection.
2.  **Retrieval**: When an agent needs to perform a task, it queries Qdrant with a natural language description of its need. Qdrant returns the most semantically similar data points, which the agent then uses to inform its actions.

## Data Management & Governance

-   **Data Lifecycle**: Automated policies will be implemented to manage the data lifecycle. For example, old, irrelevant job opportunities will be automatically archived.
-   **Backup and Recovery**: The PostgreSQL database will have a point-in-time recovery (PITR) strategy. The Qdrant database will have regular snapshots taken to ensure data can be restored in case of failure.
-   **Data Access Layer**: All database interactions will be managed through a dedicated data access layer (DAL) in the backend code. This will use an ORM (e.g., SQLAlchemy) to provide a safe and consistent way to interact with the data.

---

**Next**: Review [API Design](./04-api-design.md) for external integrations and internal service contracts.
        if profile:
            context["career_profile"] = profile
        
        # Get relevant memories
        memories = await self.memory.search_memories(
            user_id, f"context for {context_type}", limit=10
        )
        context["memories"] = memories
        
        # Add context-specific data
        if context_type == "job_search":
            context["recent_applications"] = await self._get_recent_applications(user_id)
        elif context_type == "email":
            context["email_patterns"] = await self._get_email_patterns(user_id)
        
        return context
```

## Data Pipeline Architecture

### ETL Pipelines

```python
class DataPipeline:
    def __init__(self, source_config: Dict, target_config: Dict):
        self.source = self._create_source(source_config)
        self.target = self._create_target(target_config)
        self.transformations = []
    
    def add_transformation(self, transform_func):
        """Add transformation step"""
        self.transformations.append(transform_func)
    
    async def run(self, batch_size: int = 1000):
        """Execute pipeline"""
        async for batch in self.source.read_batches(batch_size):
            # Apply transformations
            for transform in self.transformations:
                batch = await transform(batch)
            
            # Write to target
            await self.target.write_batch(batch)

# Example: Job data pipeline
class JobDataPipeline:
    def __init__(self):
        self.pipeline = DataPipeline(
            source_config={"type": "job_apis"},
            target_config={"type": "postgresql"}
        )
        
        # Add transformations
        self.pipeline.add_transformation(self._deduplicate_jobs)
        self.pipeline.add_transformation(self._standardize_format)
        self.pipeline.add_transformation(self._extract_requirements)
    
    async def _deduplicate_jobs(self, jobs: List[Dict]) -> List[Dict]:
        """Remove duplicate job postings"""
        seen = set()
        unique_jobs = []
        
        for job in jobs:
            # Create unique identifier
            identifier = f"{job['company']}_{job['title']}_{job.get('location', '')}"
            
            if identifier not in seen:
                seen.add(identifier)
                unique_jobs.append(job)
        
        return unique_jobs
    
    async def _standardize_format(self, jobs: List[Dict]) -> List[Dict]:
        """Standardize job data format"""
        standardized = []
        
        for job in jobs:
            standardized_job = {
                "title": job.get("title", "").strip(),
                "company": job.get("company", "").strip(),
                "location": self._normalize_location(job.get("location")),
                "description": job.get("description", ""),
                "salary_min": self._extract_salary_min(job.get("salary")),
                "salary_max": self._extract_salary_max(job.get("salary")),
                "remote_type": self._detect_remote_type(job.get("description", "")),
                "source": job.get("source"),
                "source_url": job.get("url"),
                "source_id": job.get("id")
            }
            standardized.append(standardized_job)
        
        return standardized
```

### Real-time Data Synchronization

```python
class RealTimeSync:
    def __init__(self, vector_db: UserMemoryManager, sql_db: DatabaseManager):
        self.vector_db = vector_db
        self.sql_db = sql_db
        self.sync_queue = asyncio.Queue()
    
    async def sync_user_update(self, user_id: str, update_type: str, data: Dict):
        """Sync user updates across databases"""
        
        # Update SQL database first (source of truth)
        await self._update_sql(user_id, update_type, data)
        
        # Update vector database for semantic search
        if update_type in ["profile_update", "preference_change"]:
            content = json.dumps(data)
            await self.vector_db.store_memory(
                user_id, f"profile_{update_type}", content
            )
        
        # Invalidate relevant caches
        await self._invalidate_cache(user_id, update_type)
    
    async def _invalidate_cache(self, user_id: str, update_type: str):
        """Invalidate related cache entries"""
        cache_patterns = [
            f"user_context:{user_id}:*",
            f"user_profile:{user_id}",
            f"recommendations:{user_id}:*"
        ]
        
        for pattern in cache_patterns:
            # Implementation depends on cache backend
            await self._delete_cache_pattern(pattern)
```

## Data Governance & Compliance

### Data Retention Policies

```python
class DataRetentionManager:
    def __init__(self, db_manager: DatabaseManager, vector_manager: UserMemoryManager):
        self.db = db_manager
        self.vector_db = vector_manager
        
        # Retention policies (in days)
        self.retention_policies = {
            "job_opportunities": 90,  # Keep job data for 3 months
            "email_classifications": 365,  # Keep email data for 1 year
            "content_generations": 180,  # Keep generated content for 6 months
            "news_articles": 30,  # Keep news articles for 1 month
            "agent_tasks": 60,  # Keep task logs for 2 months
            "user_memories": 730  # Keep user memories for 2 years
        }
    
    async def cleanup_expired_data(self):
        """Remove data based on retention policies"""
        for table, retention_days in self.retention_policies.items():
            cutoff_date = datetime.utcnow() - timedelta(days=retention_days)
            
            if table == "user_memories":
                await self._cleanup_vector_memories(cutoff_date)
            else:
                await self._cleanup_sql_table(table, cutoff_date)
    
    async def _cleanup_sql_table(self, table: str, cutoff_date: datetime):
        """Clean up SQL table data"""
        async with self.db.get_session() as session:
            if table == "job_opportunities":
                await session.execute(
                    delete(JobOpportunity).where(
                        JobOpportunity.created_at < cutoff_date,
                        JobOpportunity.status.in_(['rejected', 'archived'])
                    )
                )
            # Add other table cleanup logic
            
            await session.commit()
```

### Privacy Protection

```python
class PrivacyManager:
    def __init__(self, encryption_key: str):
        self.cipher = Fernet(encryption_key)
    
    def encrypt_sensitive_data(self, data: str) -> str:
        """Encrypt sensitive personal information"""
        return self.cipher.encrypt(data.encode()).decode()
    
    def decrypt_sensitive_data(self, encrypted_data: str) -> str:
        """Decrypt sensitive personal information"""
        return self.cipher.decrypt(encrypted_data.encode()).decode()
    
    async def anonymize_user_data(self, user_id: str):
        """Anonymize user data for analytics while preserving utility"""
        # Replace PII with hashed versions
        # Maintain referential integrity
        # Preserve analytical value
        pass
    
    async def export_user_data(self, user_id: str) -> Dict:
        """Export all user data for GDPR compliance"""
        # Collect data from all sources
        # Format for user consumption
        # Include metadata and lineage
        pass
```

---

**Next**: Review [API Design](./04-api-design.md) for external integrations and service contracts.
