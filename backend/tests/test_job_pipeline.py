#!/usr/bin/env python3
"""
Complete Job Pipeline Test

Tests the full pipeline: LinkedIn scraping ‚Üí AI job analysis ‚Üí recommendations
"""

import asyncio
import sys
import os
import json
from datetime import datetime
import pytest

# Add the backend directory to Python path
backend_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(backend_dir)

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv(os.path.join(backend_dir, '.env'))

from app.core.linkedin_scraper_v2 import linkedin_scraper_v2
from app.agents.job_agent import job_agent
import structlog

# Configure logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()


@pytest.mark.asyncio
async def test_complete_job_pipeline():
    """Test the complete job discovery and analysis pipeline."""
    
    print("üîÑ Testing Complete Job Pipeline")
    print("=" * 60)
    
    try:
        # Step 1: Scrape jobs from LinkedIn
        print("\n1Ô∏è‚É£ Scraping jobs from LinkedIn (60s timeout)...")
        jobs = await asyncio.wait_for(
            linkedin_scraper_v2.search_jobs(
                keywords="Python Developer",
                location="Remote",
                limit=3  # Small number for testing
            ),
            timeout=60.0  # 60 second timeout for scraping
        )
        
        if not jobs:
            print("‚ùå No jobs were scraped - cannot continue with pipeline test")
            return False
        
        print(f"‚úÖ Scraped {len(jobs)} jobs successfully")
        
        # Step 2: Test AI analysis on first job
        print("\n2Ô∏è‚É£ Testing AI job analysis...")
        first_job = jobs[0]
        print(f"   Analyzing: '{first_job.get('title')}' at '{first_job.get('company')}'")
        
        # Create a simple user profile for testing the AI analysis
        test_user_profile = {
            "skills": ["Python", "Django", "FastAPI", "PostgreSQL"],
            "experience_level": "mid",
            "preferred_locations": ["Remote", "San Francisco"],
            "job_types": ["full-time"],
            "salary_range": {"min": 80000, "max": 150000}
        }
        
        # Use the actual job agent method for scoring with timeout
        print("üîÑ Running AI analysis (30s timeout)...")
        analysis = await asyncio.wait_for(
            job_agent._score_single_job(first_job, test_user_profile),
            timeout=30.0
        )
        
        if analysis:
            print("‚úÖ AI job analysis completed successfully")
            print(f"   Relevance Score: {analysis.get('relevance_score', 'N/A')}")
            ai_analysis = analysis.get('ai_analysis', {})
            print(f"   AI Analysis Keys: {list(ai_analysis.keys())}")
            if 'skills_required' in ai_analysis:
                print(f"   Skills Required: {len(ai_analysis.get('skills_required', []))} skills identified")
            if 'pros' in ai_analysis:
                print(f"   Pros: {len(ai_analysis.get('pros', []))} positive points")
            if 'cons' in ai_analysis:
                print(f"   Cons: {len(ai_analysis.get('cons', []))} concerns")
        else:
            print("‚ùå AI job analysis failed")
            return False
        
        # Step 3: Test job discovery pipeline 
        print("\n3Ô∏è‚É£ Testing job discovery pipeline...")
        
        # Test the main discovery method with a test user ID
        test_user_id = "test_user_123"
        
        # Create search parameters
        search_params = {
            "keywords": "Python Developer",
            "location": "Remote",
            "limit": 3
        }
        
        # Note: This will try to use the database, so we'll catch any DB errors gracefully
        try:
            print("üîÑ Running job discovery pipeline (45s timeout)...")
            discovery_result = await asyncio.wait_for(
                job_agent.discover_jobs_for_user(
                    user_id=test_user_id,
                    search_params=search_params,
                    force_refresh=True
                ),
                timeout=45.0  # 45 second timeout
            )
            
            if discovery_result and not discovery_result.get('error'):
                print(f"‚úÖ Job discovery pipeline completed")
                print(f"   Jobs Found: {discovery_result.get('jobs_found', 0)}")
                print(f"   Qualified Jobs: {discovery_result.get('qualified_jobs', 0)}")
            else:
                print("‚ö†Ô∏è Job discovery completed with issues (likely database connection)")
                print(f"   Result: {discovery_result}")
        
        except asyncio.TimeoutError:
            print(f"‚ö†Ô∏è Job discovery pipeline timed out after 45 seconds")
            print("   This could indicate database connection issues")
        except Exception as e:
            print(f"‚ö†Ô∏è Job discovery pipeline test skipped due to DB dependency: {e}")
            print("   This is expected in a standalone test environment")
        
        # Step 4: Save results for inspection
        print("\n4Ô∏è‚É£ Saving pipeline results...")
        
        pipeline_results = {
            "timestamp": datetime.now().isoformat(),
            "scraped_jobs_count": len(jobs),
            "scraped_jobs": jobs,
            "sample_analysis": analysis,
            "user_profile": test_user_profile
        }
        
        results_file = f"job_pipeline_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(results_file, 'w') as f:
            json.dump(pipeline_results, f, indent=2, default=str)
        
        print(f"‚úÖ Results saved to: {results_file}")
        
        print("\nüéâ Complete pipeline test successful!")
        print("=" * 60)
        print("SUMMARY:")
        print(f"  ‚úÖ Scraped {len(jobs)} jobs")
        print(f"  ‚úÖ AI analysis working")
        print(f"  ‚úÖ Job discovery pipeline tested")
        print(f"  ‚úÖ Results saved to {results_file}")
        
        return True
        
    except asyncio.TimeoutError:
        print(f"\n‚ùå Pipeline test timed out")
        print("   One of the API calls (LinkedIn scraping or AI analysis) took too long")
        print("   This could indicate network issues or service overload")
        return False
    except Exception as e:
        print(f"\n‚ùå Pipeline test failed: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        await linkedin_scraper_v2.close()


@pytest.mark.asyncio
async def test_ai_configuration():
    """Test if AI services are properly configured."""
    
    print("\nü§ñ Testing AI Configuration...")
    
    # Check if Gemini API is configured
    from app.core.config import settings
    
    if hasattr(settings, 'GOOGLE_GEMINI_API_KEY') and settings.GOOGLE_GEMINI_API_KEY:
        print("‚úÖ Google Gemini API key configured")
    else:
        print("‚ùå Google Gemini API key not configured")
        print("   Please set GOOGLE_GEMINI_API_KEY in your environment")
        return False
    
    # Test a simple AI call with timeout
    try:
        test_job = {
            "title": "Python Developer",
            "company": "Test Company",
            "description": "Looking for a Python developer with experience in web development.",
            "location": "Remote"
        }
        
        test_user_profile = {
            "skills": ["Python", "Django"],
            "experience_level": "mid"
        }
        
        print("üîÑ Making live AI API call (30s timeout)...")
        
        # Add timeout to the AI call
        analysis = await asyncio.wait_for(
            job_agent._score_single_job(test_job, test_user_profile),
            timeout=30.0  # 30 second timeout
        )
        
        if analysis and isinstance(analysis, dict) and 'relevance_score' in analysis:
            print("‚úÖ AI analysis service is working")
            print(f"   Test relevance score: {analysis.get('relevance_score')}")
            return True
        else:
            print("‚ùå AI analysis returned invalid response")
            print(f"   Response: {analysis}")
            return False
            
    except asyncio.TimeoutError:
        print(f"‚ùå AI service test timed out after 30 seconds")
        print("   This could indicate network issues or API rate limiting")
        return False
    except Exception as e:
        print(f"‚ùå AI service test failed: {e}")
        return False


if __name__ == "__main__":
    async def main():
        print("üß™ Complete Job Pipeline Test Suite")
        print("=" * 50)
        
        # First test AI configuration
        ai_ok = await test_ai_configuration()
        if not ai_ok:
            print("\n‚ùå AI services not properly configured")
            return
        
        # Then test the complete pipeline
        success = await test_complete_job_pipeline()
        
        if success:
            print("\nüéâ Complete job pipeline is working correctly!")
        else:
            print("\nüí• Pipeline test failed - check configuration and logs")
    
    asyncio.run(main())